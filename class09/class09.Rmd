---
title: "Unsupervised Mini Learning Project"
author: "Brie Diaz"
date: "5/1/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Open Data Set:

```{r}
wisc.df <- read.csv("WisconsinCancer.csv")
head(wisc.df)
```


Q1: What type of object is returned from the read.csv() function? 
```{r}
is.data.frame(wisc.df)
```
A1: data frame

Q2: How many observations [i.e. patients] are in this dataset?
```{r}
nrow(wisc.df)
```
A2: 569

Q3: How many of the observations have a malignant diagnosis?
```{r}
table(wisc.df$diagnosis)
```
A3: 212


Q4: How many vairiables/features in the data are suffixed with _mean?

```{r}
colnames(wisc.df)
length(grep("_mean", colnames(wisc.df)))
```

10!

```{r}
head(wisc.df)
```

Select columns 3:32 and store it in a matrix
```{r}
wisc.data <- as.matrix(wisc.df[,3:32])

```


Set the row names of wisc.data
```{r}
row.names(wisc.data) <- wisc.df$id
```


Set up a separate new vector called diagnosis that contains the data from the diagnosis column of the original data set

```{r}
diagnosis <- wisc.df$diagnosis
```


Perform PCR on the data set 



Check the means and standard deviations of the features (columns) of the wisc.data
```{r}
round(colMeans(wisc.data), 1)

```



```{r}
apply(wisc.data,2,sd)
```


Q6: Do we need to scale the data?
A6: yes, we do. The mean and standard deviation values for each column are vastly different/variable.

Now apply the prcomp() function, and be sure to scale! 
```{r}
wisc.pr <- prcomp(wisc.data, scale=TRUE)
summary(wisc.pr)
```


Create a plot to visualize the PCA model
```{r}
biplot(wisc.pr)
```

Create a scatter plot to visualize observations by components 1 and 2
```{r}
plot(wisc.pr$x, col = diagnosis, xlab="PC1", ylab="PC2")
```

OR you can do this code:
```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,2], col = diagnosis, xlab="PC1", ylab="PC2")
```






Repeat for components 1 and 3
```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,3], col = diagnosis, xlab="PC1", ylab="PC3")
```
What do components 2 and 3 look like?

```{r}
plot(wisc.pr$x[,2], wisc.pr$x[,3], col = diagnosis, xlab="PC2", ylab="PC3")
```
a lot more jumbled! 


Calculate variance of each component

```{r}
pr.var <- wisc.pr$sdev^2
head(pr.var)
```



Calculate the variance explained by each principal component by dividing by the total variance explained of all principal components. 
```{r}
pve <- pr.var/sum(pr.var)
```

Plot the variance explained by each principal component
```{r}
plot(pve, xlab= "Principle Component", ylab="Proportion of Variance Explained", ylim = c(0,1), type="o")
```






Analyze the variance: make a scree plot of this same data

```{r}
barplot(pve, ylab = "Percent of Variance Explained", names.arg=paste0("PC", 1:length(pve)), las=2, axes=FALSE)
axis(2, at=pve, labels=round(pve,2)*100)
```


Make a pretty ggplot baed graph

```{r}
#install.packages("factoextra")
library(factoextra)
fviz_eig(wisc.pr, addlabels = TRUE)
```


Q13: For the first principal component, and using two significant figures, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature radius_mean?


```{r}
wisc.pr$rotation[,1]
```

A13: -0.22

Q14: For the first principal component, and using two significant figures, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature of smoothness_se?



```{r}
wisc.pr$rotation[,1]
```

A14: -0.01


Q15: Which of the original variables contributes most to PC1?
  
```{r}
sort(abs(wisc.pr$rotation[,1]))
```

#Hierarchical Clustering



Scale the wisc.data
```{r}
data.scaled <- scale(wisc.data)
```

Calculate the Euclidean distances between all pairs of observatins in the new scaled dataset and assign the result to data.dist


```{r}
data.dist <- dist(data.scaled)
```



Create a hierarchical clustering model using complete linkage. 

```{r}
wisc.hclust <- hclust(data.dist)
wisc.hclust
```



Q16: Using the plot() and abline() functions, whta is the height at which the clustering model has 4 clusters?

```{r}
plot(wisc.hclust)
abline(wisc.hclust, col="blue", lty=2, h=19)
```



Cut the tree so that it has 4 clusters

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k=4)
table(wisc.hclust.clusters)
```


```{r}
table(wisc.hclust.clusters, diagnosis)

```




## Section 5. Combining methods (PCA & hclust)


I am going to start wit hthe PCs that capture 90% of the original variane in the dataset (i.e. PC1 to PC7)
```{r}
wisc.pr.hclust <- hclust(dist(wisc.pr$x[,1:7]), method = "ward.D2")
```

```{r}
plot(wisc.pr.hclust)
abline(h=70, col="red", lty=2)
```

```{r}
grps <- cutree(wisc.pr.hclust, k=2)
table(grps)
```

```{r}
table(grps, diagnosis)
```



```{r}
plot(wisc.pr$x[,1:2], col=grps)
```

```{r}
#install.packages("rgl")
library(rgl)
plot3d(wisc.pr$x[,1:3], xlab="PC 1", ylab="PC 2", zlab="PC 3", cex=1.5, size=1, type="s", col=grps)
rglwidget(width = 400, height = 400) 
```







## Making Predictions 

We will use the predict() function that will take our PCA model from before and new cancer cell data and project that data onto out PCA space 

```{r}
new <- read.csv("new_samples.csv")
npc <- predict(wisc.pr, newdata=new)
npc

```

```{r}
g <- as.factor(grps)
plot(wisc.pr$x[,1:2], col =g)
points(npc[,1], npc[,2], col="blue", pch=16, cex=3)
text(npc[,1], npc[,2], c(1,2), col="white")
```


Q22: Which of these new patients should we prioritize for follow up based on your results?
A22: patient 2; this person is clustered along with other patients diagnosed with malignant cancer













